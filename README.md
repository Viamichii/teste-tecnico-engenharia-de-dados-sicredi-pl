# ğŸ¦ Desafio TÃ©cnico â€“ Engenharia de Dados (Sicredi)

Pipeline ETL completa usando SQL Server, PySpark, Python e Docker

Este projeto implementa uma pipeline completa de ingestÃ£o e transformaÃ§Ã£o de dados, simulando um fluxo real de engenharia de dados utilizado em ambientes corporativos.

## O objetivo Ã© demonstrar:

- CriaÃ§Ã£o e organizaÃ§Ã£o de um ambiente de dados
- GeraÃ§Ã£o automÃ¡tica de dados transacionais
- Leitura a partir de um banco SQL Server
- Processamento Bronze â†’ Silver com PySpark
- TransformaÃ§Ãµes, normalizaÃ§Ã£o e flatten
- Salvamento em Parquet (Bronze) e CSV (Silver)
- ExecuÃ§Ã£o automatizada em Docker (bonus)

## ğŸ“ Arquitetura do Projeto

```
teste-tecnico-engenharia-de-dados-sicredi-pl/
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql               # CriaÃ§Ã£o das tabelas e usuÃ¡rio
â”‚   â””â”€â”€ data_generator.py        # GeraÃ§Ã£o dos dados fictÃ­cios no SQL Server
â”‚
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ etl_sicooperative.py     # Pipeline completa (Bronze + Silver)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/                  # Armazena Parquet
â”‚   â””â”€â”€ silver/                  # Armazena CSV final
â”‚
â”œâ”€â”€ configs.py                   # ConfiguraÃ§Ãµes globais
â”œâ”€â”€ Dockerfile                   # Executa a ETL no container
â”œâ”€â”€ docker-compose.yml           # Subir SQL Server + ETL
â””â”€â”€ README.md                    # Este arquivo
```

## ğŸ§  LÃ³gica do Projeto (Pipeline)

O pipeline foi projetado simulando uma arquitetura de dados moderna, semelhante Ã  utilizada em squads de engenharia, seguindo os padrÃµes Bronze â†’ Silver.

### 1ï¸âƒ£ Camada SQL (Fonte dos Dados)

O arquivo `schema.sql` cria as tabelas:
- associado
- conta
- cartao
- movimento

**ObservaÃ§Ã£o:** NÃ£o foi possÃ­vel criar a coluna data de criaÃ§Ã£o do cartÃ£o, pois ela nÃ£o existe no modelo fornecido.

### 2ï¸âƒ£ GeraÃ§Ã£o de Dados FictÃ­cios (data_generator.py)

- Gera 100 associados aleatÃ³rios
- Para cada associado:
  - Cria conta
  - Cria cartÃ£o
  - Gera 3â€“8 movimentos
- Usa Faker + pyodbc
- Apenas insere dados vÃ¡lidos (PK autoincremento garante nÃ£o duplicaÃ§Ã£o)

Esse script simula um ambiente produtivo recebendo dados transacionais.

### 3ï¸âƒ£ Camada Bronze (Parquet)

O PySpark lÃª diretamente o SQL Server usando JDBC:
- associado
- conta
- cartao
- movimento

E salva em: `data/bronze/<tabela>/*.parquet`

A Bronze Ã© sempre overwrite, imitando cargas full prÃ³ximas do mundo real.

### 4ï¸âƒ£ Camada Silver (TransformaÃ§Ã£o Final)

OperaÃ§Ãµes aplicadas:
- Joins entre as quatro tabelas
- Flatten para formato analÃ­tico
- NormalizaÃ§Ã£o
- Cast de tipos para strings (exigido no desafio)

GeraÃ§Ã£o de CSV Ãºnico: `data/silver/sicredi_movimentos.csv`

## ğŸš€ ExecuÃ§Ã£o Completa Automatizada (Pipeline Ãšnica)

O arquivo `etl_sicooperative.py` orquestra:
1. CriaÃ§Ã£o da SparkSession
2. ExecuÃ§Ã£o do data_generator
3. GeraÃ§Ã£o da Bronze
4. GeraÃ§Ã£o da Silver
5. Fechamento da sessÃ£o Spark

Esse script Ã© usado pelo Docker como ponto de entrada para rodar tudo automaticamente.

## ğŸ³ ExecuÃ§Ã£o com Docker (BÃ´nus do Desafio)

O ambiente foi containerizado com `docker-compose.yml`, que sobe:
- Um container SQL Server
- Um container Python que executa a pipeline completa

### Para rodar:

```bash
docker compose up --build
```

Durante a execuÃ§Ã£o:
1. O SQL Server Ã© iniciado
2. Gera dados fictÃ­cios
3. Produz Bronze
4. Produz Silver

## ğŸ›  Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| SQL Server | Fonte transacional |
| PySpark | Processamento distribuÃ­do |
| Python | OrquestraÃ§Ãµes, geraÃ§Ã£o de dados |
| Docker | AutomaÃ§Ã£o e provisionamento |
| Faker | GeraÃ§Ã£o de dados simulados |
| Parquet | Armazenamento Bronze |
| CSV | Entrega Silver |

## ğŸ“Œ ObservaÃ§Ãµes Importantes

- Estamos simulando um sistema real, onde a aplicaÃ§Ã£o consome dados armazenados em SQL Server.
- O Docker representa um cenÃ¡rio de ambiente separado (como ProduÃ§Ã£o x Desenvolvimento).
- A coluna data de criaÃ§Ã£o do cartÃ£o nÃ£o pÃ´de ser implementada porque nÃ£o existe no modelo fornecido.

## ğŸ§ª Como Executar Localmente

### 1. Criar ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Rodar somente a ETL:
```bash
python etl/etl_sicooperative.py
```

### 3. Rodar tudo com Docker:
```bash
docker compose up --build
```

## ğŸ Como executar o ETL â€” passo a passo

1) Rodar localmente (venv)
- Criar e ativar virtualenv:
  - Windows:
    - python -m venv venv
    - venv\Scripts\activate
  - Linux/macOS:
    - python -m venv venv
    - source venv/bin/activate
- Instalar dependÃªncias:
  - pip install -r requirements.txt
- Verificar configs:
  - Ajuste variÃ¡veis em configs.py ou via env vars (DB_HOST, DB_PORT, DB_USER, DB_PASSWORD).
- Rodar gerador de dados (opcional, popula o banco):
  - python sql/data_generator.py
- Rodar pipeline completa:
  - python etl/etl_sicooperative.py

2) Rodar com Docker (recomendado para reproducibilidade)
- Subir todo o ambiente (SQL Server + ETL):
  - docker compose up --build
- Executar apenas o ETL (apÃ³s subir o SQL Server):
  - docker compose up -d sqlserver
  - docker compose run --rm etl python etl/etl_sicooperative.py
- Ver logs:
  - docker compose logs -f etl
  - docker compose logs -f sqlserver

3) VerificaÃ§Ãµes e troubleshooting rÃ¡pido
- A ETL falha por conexÃ£o:
  - Verifique se o SQL Server estÃ¡ healthy (checar logs e healthcheck do compose).
  - Confirme credenciais e host em configs.py / variÃ¡veis de ambiente.
- Erro ODBC / pyodbc:
  - Dentro do container, teste conexÃ£o via um pequeno script pyodbc ou rodando sql/data_generator.py isolado.
  - Se faltar driver ODBC, considere instalar `unixodbc`/driver apropriado ou ajustar Dockerfile, mas prefira a imagem sugerida (bitnami/spark) e unixodbc.
- Se o build Docker demora:
  - Use a imagem base com PySpark prÃ©-instalado (jÃ¡ configurada no Dockerfile do projeto).
  - Evite reinstalar pyspark no pip dentro do container.
- Teste incremental:
  - Primeiro execute sql/data_generator.py para verificar inserÃ§Ã£o no DB.
  - Depois execute apenas a parte de leitura (ex.: rodar um script que lÃª uma tabela via pandas/pyodbc).

4) Dicas finais
- Para depurar rapidamente, rode os scripts localmente fora do Docker (isso isola problemas de driver/build).
- Ajuste spark.conf ("spark.sql.shuffle.partitions") no etl_sicooperative.py para volumes pequenos (jÃ¡ definido para 1 no projeto).

## âœ”ï¸ Resultado Final

Ao final da execuÃ§Ã£o vocÃª terÃ¡:

### ğŸ“ Bronze
Parquets organizados por tabela.

### ğŸ“ Silver
CSV analÃ­tico final contendo os movimentos flatenados.

### ğŸ¯ Pipeline concluÃ­da de ponta a ponta
Simulando um ambiente real com:
- Dados transacionais
- Processamento estruturado
- Workflow completo
- AutomaÃ§Ã£o via Docker