# ğŸ¦ Desafio TÃ©cnico â€“ Engenharia de Dados (Sicredi)

Pipeline ETL completa usando SQL Server, PySpark, Python e Docker

Este projeto implementa uma pipeline completa de ingestÃ£o e transformaÃ§Ã£o de dados, simulando um fluxo real de engenharia de dados utilizado em ambientes corporativos.

## ğŸ“ ObservaÃ§Ãµes (como solicitado no desafio)

### Por que optei por este design?

Escolhi estruturar o projeto com camadas Bronze â†’ Silver, aplicando conceitos de pipelines modernas (Databricks/Lakehouse).
Assim, o fluxo fica organizado, escalÃ¡vel, testÃ¡vel e semelhante a ambientes reais.

**Processo de produÃ§Ã£o:**
O fluxo segue a sequÃªncia: **Tabelas â†’ GeraÃ§Ã£o de dados â†’ Bronze (Parquet) â†’ Silver (CSV)**

No inÃ­cio, criei manualmente o banco, usuÃ¡rio e tabelas. Depois, com a automaÃ§Ã£o, o Docker cuida de todo esse setup automaticamente, garantindo reproducibilidade e facilitando o teste em diferentes ambientes.

### O que faria se tivesse mais tempo?

- Solucionaria o problema no docker e comecaria o projeto por ele
- Criaria testes unitÃ¡rios para validaÃ§Ã£o de cada etapa
- Construiria um BI consumindo o CSV Silver
- Automatizaria a criaÃ§Ã£o do usuÃ¡rio SQL (sicredi_user) diretamente no Docker

### Dificuldades encontradas

**SQL Server + Docker:**
Usei Microsoft SQL Server, que roda nativamente em Windows, mas o container oficial utiliza Linux.
Isso exigiu atenÃ§Ã£o extra na integraÃ§Ã£o, especialmente nos drivers JDBC/ODBC.

**CriaÃ§Ã£o do usuÃ¡rio `sicredi_user`:**
A criaÃ§Ã£o automÃ¡tica do usuÃ¡rio no primeiro login nÃ£o funcionou como esperado dentro do Docker.
Recomendo que o avaliador crie o usuÃ¡rio manualmente antes de testar, garantindo melhor performance na ETL:

```sql
CREATE LOGIN sicredi_user WITH PASSWORD = 'SenhaForte123!';
CREATE USER sicredi_user FOR LOGIN sicredi_user;
ALTER ROLE db_owner ADD MEMBER sicredi_user;
```

**Tempo de build:**
O tempo de build do Docker Ã© maior que o normal, pois Spark precisa ser instalado dentro da imagem.

## O objetivo Ã© demonstrar:

- CriaÃ§Ã£o e organizaÃ§Ã£o de um ambiente de dados
- GeraÃ§Ã£o automÃ¡tica de dados transacionais
- Leitura a partir de um banco SQL Server
- Processamento Bronze â†’ Silver com PySpark
- Salvamento em Parquet (Bronze) e CSV (Silver)
- ExecuÃ§Ã£o automatizada em Docker (bonus)

## ğŸ“ Arquitetura do Projeto

```
teste-tecnico-engenharia-de-dados-sicredi-pl/
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql               # CriaÃ§Ã£o das tabelas e usuÃ¡rio
â”‚   â””â”€â”€ data_generator.py        # GeraÃ§Ã£o dos dados fictÃ­cios no SQL Server
â”‚
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ etl_sicooperative.py     # Pipeline completa (Bronze + Silver)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/                  # Armazena Parquet
â”‚   â””â”€â”€ silver/                  # Armazena CSV final
â”‚
â”œâ”€â”€ configs.py                   # ConfiguraÃ§Ãµes globais
â”œâ”€â”€ Dockerfile                   # Executa a ETL no container
â”œâ”€â”€ docker-compose.yml           # Subir SQL Server + ETL
â””â”€â”€ README.md                    # Este arquivo
```

## ğŸ§  LÃ³gica do Projeto (Pipeline)

O pipeline foi projetado simulando uma arquitetura de dados moderna, semelhante Ã  utilizada em squads de engenharia, seguindo os padrÃµes Bronze â†’ Silver.

### 1ï¸âƒ£ Camada SQL (Fonte dos Dados)

O arquivo `schema.sql` cria as tabelas:
- associado
- conta
- cartao
- movimento

**ObservaÃ§Ã£o:** NÃ£o foi possÃ­vel criar a coluna data de criaÃ§Ã£o do cartÃ£o, pois ela nÃ£o existe no modelo fornecido.

### 2ï¸âƒ£ GeraÃ§Ã£o de Dados FictÃ­cios (data_generator.py)

- Gera 100 associados aleatÃ³rios
- Para cada associado:
  - Cria conta
  - Cria cartÃ£o
  - Gera 3â€“8 movimentos
- Usa Faker + pyodbc
- Apenas insere dados vÃ¡lidos (PK autoincremento garante nÃ£o duplicaÃ§Ã£o)

Esse script simula um ambiente produtivo recebendo dados transacionais.

### 3ï¸âƒ£ Camada Bronze (Parquet)

O PySpark lÃª diretamente o SQL Server usando JDBC:
- associado
- conta
- cartao
- movimento

E salva em: `data/bronze/<tabela>/*.parquet`

A Bronze Ã© sempre overwrite, imitando cargas full prÃ³ximas do mundo real.

### 4ï¸âƒ£ Camada Silver (TransformaÃ§Ã£o Final)

OperaÃ§Ãµes aplicadas:
- Joins entre as quatro tabelas
- Flatten para formato analÃ­tico
- NormalizaÃ§Ã£o
- Cast de tipos para strings (exigido no desafio)

GeraÃ§Ã£o de CSV Ãºnico: `data/silver/sicredi_movimentos.csv`

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Localmente com Python venv

#### 1. Criar e ativar o ambiente virtual:

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/macOS:**
```bash
python -m venv venv
source venv/bin/activate
```

#### 2. Instalar dependÃªncias:

```bash
pip install -r requirements.txt
```

> **ğŸ“Œ Nota:** As etapas 3, 4 e 5 referem-se Ã  configuraÃ§Ã£o do banco de dados (criar banco, criar tabelas e verificar configuraÃ§Ãµes) e jÃ¡ estÃ£o **automatizadas no script `etl_sicooperative.py`**. Elas sÃ£o apresentadas no passo a passo por garantia de execuÃ§Ã£o caso vocÃª prefira configurar manualmente ou precise de troubleshooting.

#### 3. Configurar banco de dados SQL Server:

Execute os comandos no SQL Server Management Studio ou sqlcmd para criar o banco e o usuÃ¡rio:

```sql
-- Criar banco de dados
CREATE DATABASE sicredi;
GO

USE sicredi;
GO

-- Criar login e usuÃ¡rio
CREATE LOGIN sicredi_user WITH PASSWORD = 'SenhaForte123!';
GO

CREATE USER sicredi_user FOR LOGIN sicredi_user;
GO

ALTER ROLE db_owner ADD MEMBER sicredi_user;
GO
```

#### 4. Criar as tabelas:

Execute o script SQL para criar as tabelas:

```bash
# Windows (usando sqlcmd)
sqlcmd -S localhost -U sicredi_user -P SenhaForte123! -d sicredi -i sql/schema.sql
```

Ou execute o conteÃºdo de `sql/schema.sql` no SQL Server Management Studio.

#### 5. Verificar configuraÃ§Ãµes:

Ajuste as variÃ¡veis em `configs.py` ou configure via variÃ¡veis de ambiente:
- `DB_HOST` (padrÃ£o: localhost)
- `DB_PORT` (padrÃ£o: 1433)
- `DB_USER` (padrÃ£o: sicredi_user)
- `DB_PASSWORD`

#### 6. Executar a pipeline ETL completa:

```bash
python etl/etl_sicooperative.py
```

A ETL irÃ¡:
1. Conectar ao SQL Server
2. Gerar dados fictÃ­cios automaticamente
3. Criar a camada Bronze (Parquet)
4. Criar a camada Silver (CSV)
5. Salvar em `data/bronze/` e `data/silver/`

---

### OpÃ§Ã£o 2: Com Docker Compose (Recomendado)

O Docker automatiza todo o ambiente, subindo SQL Server + ETL em containers.

#### 1. Construir e executar:

```bash
docker compose up --build
```

Durante a execuÃ§Ã£o:
1. âœ… SQL Server Ã© inicializado
2. âœ… Tabelas sÃ£o criadas automaticamente
3. âœ… Dados fictÃ­cios sÃ£o gerados
4. âœ… Pipeline Bronze Ã© produzida
5. âœ… Pipeline Silver Ã© produzida
6. âœ… Logs sÃ£o exibidos no console

#### 2. Executar componentes isolados:

Subir apenas o SQL Server (manter em background):
```bash
docker compose up -d sqlserver
```

Depois executar a ETL:
```bash
docker compose run --rm etl python etl/etl_sicooperative.py
```

#### 3. Ver logs:

```bash
# Logs da ETL
docker compose logs -f etl

# Logs do SQL Server
docker compose logs -f sqlserver
```

#### 4. Parar os containers:

```bash
docker compose down
```


---

## ğŸ›  Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| SQL Server | Fonte transacional |
| PySpark | Processamento distribuÃ­do |
| Python | OrquestraÃ§Ãµes, geraÃ§Ã£o de dados |
| Docker | AutomaÃ§Ã£o e provisionamento |
| Faker | GeraÃ§Ã£o de dados simulados |
| Parquet | Armazenamento Bronze |
| CSV | Entrega Silver |

## ğŸ” Troubleshooting

### A ETL falha por conexÃ£o com SQL Server
- **venv:** Verifique se o SQL Server estÃ¡ rodando localmente
- **Docker:** Aguarde alguns segundos para o SQL Server inicializar (verifique com `docker compose logs sqlserver`)
- Confirme credenciais em `configs.py` ou variÃ¡veis de ambiente

### Erro ODBC / pyodbc
- erro entre conexao do sqlserver e ubunto no docker, erro se refere ao conector e driver que o ubunto nao tem.

## ğŸ“Œ ObservaÃ§Ãµes Importantes

- Estamos simulando um sistema real, onde a aplicaÃ§Ã£o consome dados armazenados em SQL Server
- O Docker representa um cenÃ¡rio de ambiente separado (como ProduÃ§Ã£o x Desenvolvimento)
- A coluna data de criaÃ§Ã£o do cartÃ£o nÃ£o pÃ´de ser implementada porque nÃ£o existe no modelo fornecido

### âš ï¸ Tempo de Build do Docker

![aed64771-d31d-4741-9e98-66140dad58a3](https://github.com/user-attachments/assets/558df58c-e485-4e66-ba17-4ac4a541e916)


> **Nota:** Sempre que repetia o build no Docker, o processo demorava muito a ponto de parecer que estava travado. Ao rodar localmente novamente depois de clonar o projeto, notei que o Spark demorava bastante para baixar e configurar. Esse Ã© provavelmente o motivo da demora no build do Docker. Tenha paciÃªncia durante o primeiro build - pode levar vÃ¡rios minutos.

