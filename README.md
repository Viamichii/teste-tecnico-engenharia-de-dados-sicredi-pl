# üè¶ Desafio T√©cnico ‚Äì Engenharia de Dados (Sicredi)

Pipeline ETL completa usando SQL Server, PySpark, Python e Docker

Este projeto implementa uma pipeline completa de ingest√£o e transforma√ß√£o de dados, simulando um fluxo real de engenharia de dados utilizado em ambientes corporativos.

## üìù Observa√ß√µes (como solicitado no desafio)

### Por que optei por este design?

Escolhi estruturar o projeto com camadas Bronze ‚Üí Silver, aplicando conceitos de pipelines modernas (Databricks/Lakehouse).
Assim, o fluxo fica organizado, escal√°vel, test√°vel e semelhante a ambientes reais.

### O que faria se tivesse mais tempo?

- Solucionario o problema no docker e comecaria o projeto por ele
- Criaria testes unit√°rios para valida√ß√£o de cada etapa
- Construiria um BI consumindo o CSV Silver
- Automatizaria a cria√ß√£o do usu√°rio SQL (sicredi_user) diretamente no Docker

### Dificuldades encontradas

**SQL Server + Docker:**
Usei Microsoft SQL Server, que roda nativamente em Windows, mas o container oficial utiliza Linux.
Isso exigiu aten√ß√£o extra na integra√ß√£o, especialmente nos drivers JDBC/ODBC.

**Cria√ß√£o do usu√°rio `sicredi_user`:**
A cria√ß√£o autom√°tica do usu√°rio no primeiro login n√£o funcionou como esperado dentro do Docker.
Recomendo que o avaliador crie o usu√°rio manualmente antes de testar, garantindo melhor performance na ETL:

```sql
CREATE LOGIN sicredi_user WITH PASSWORD = 'SenhaForte123!';
CREATE USER sicredi_user FOR LOGIN sicredi_user;
ALTER ROLE db_owner ADD MEMBER sicredi_user;
```

**Tempo de build:**
O tempo de build do Docker √© maior que o normal, pois Spark precisa ser instalado dentro da imagem.

## O objetivo √© demonstrar:

- Cria√ß√£o e organiza√ß√£o de um ambiente de dados
- Gera√ß√£o autom√°tica de dados transacionais
- Leitura a partir de um banco SQL Server
- Processamento Bronze ‚Üí Silver com PySpark
- Transforma√ß√µes, normaliza√ß√£o e flatten
- Salvamento em Parquet (Bronze) e CSV (Silver)
- Execu√ß√£o automatizada em Docker (bonus)

## üìÅ Arquitetura do Projeto

```
teste-tecnico-engenharia-de-dados-sicredi-pl/
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql               # Cria√ß√£o das tabelas e usu√°rio
‚îÇ   ‚îî‚îÄ‚îÄ data_generator.py        # Gera√ß√£o dos dados fict√≠cios no SQL Server
‚îÇ
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îî‚îÄ‚îÄ etl_sicooperative.py     # Pipeline completa (Bronze + Silver)
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/                  # Armazena Parquet
‚îÇ   ‚îî‚îÄ‚îÄ silver/                  # Armazena CSV final
‚îÇ
‚îú‚îÄ‚îÄ configs.py                   # Configura√ß√µes globais
‚îú‚îÄ‚îÄ Dockerfile                   # Executa a ETL no container
‚îú‚îÄ‚îÄ docker-compose.yml           # Subir SQL Server + ETL
‚îî‚îÄ‚îÄ README.md                    # Este arquivo
```

## üß† L√≥gica do Projeto (Pipeline)

O pipeline foi projetado simulando uma arquitetura de dados moderna, semelhante √† utilizada em squads de engenharia, seguindo os padr√µes Bronze ‚Üí Silver.

### 1Ô∏è‚É£ Camada SQL (Fonte dos Dados)

O arquivo `schema.sql` cria as tabelas:
- associado
- conta
- cartao
- movimento

**Observa√ß√£o:** N√£o foi poss√≠vel criar a coluna data de cria√ß√£o do cart√£o, pois ela n√£o existe no modelo fornecido.

### 2Ô∏è‚É£ Gera√ß√£o de Dados Fict√≠cios (data_generator.py)

- Gera 100 associados aleat√≥rios
- Para cada associado:
  - Cria conta
  - Cria cart√£o
  - Gera 3‚Äì8 movimentos
- Usa Faker + pyodbc
- Apenas insere dados v√°lidos (PK autoincremento garante n√£o duplica√ß√£o)

Esse script simula um ambiente produtivo recebendo dados transacionais.

### 3Ô∏è‚É£ Camada Bronze (Parquet)

O PySpark l√™ diretamente o SQL Server usando JDBC:
- associado
- conta
- cartao
- movimento

E salva em: `data/bronze/<tabela>/*.parquet`

A Bronze √© sempre overwrite, imitando cargas full pr√≥ximas do mundo real.

### 4Ô∏è‚É£ Camada Silver (Transforma√ß√£o Final)

Opera√ß√µes aplicadas:
- Joins entre as quatro tabelas
- Flatten para formato anal√≠tico
- Normaliza√ß√£o
- Cast de tipos para strings (exigido no desafio)

Gera√ß√£o de CSV √∫nico: `data/silver/sicredi_movimentos.csv`

## üöÄ Como Executar

### Op√ß√£o 1: Localmente com Python venv

#### 1. Criar e ativar o ambiente virtual:

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/macOS:**
```bash
python -m venv venv
source venv/bin/activate
```

#### 2. Instalar depend√™ncias:

```bash
pip install -r requirements.txt
```

#### 3. Verificar configura√ß√µes:

Ajuste as vari√°veis em `configs.py` ou configure via vari√°veis de ambiente:
- `DB_HOST` (padr√£o: localhost)
- `DB_PORT` (padr√£o: 1433)
- `DB_USER` (padr√£o: sicredi_user)
- `DB_PASSWORD`

#### 4. Popular o banco de dados (opcional):

Se o SQL Server estiver rodando localmente:
```bash
python sql/data_generator.py
```

#### 5. Executar a pipeline ETL completa:

```bash
python etl/etl_sicooperative.py
```

Isso ir√°:
1. Conectar ao SQL Server
2. Gerar dados fict√≠cios (se necess√°rio)
3. Criar a camada Bronze (Parquet)
4. Criar a camada Silver (CSV)
5. Salvar em `data/bronze/` e `data/silver/`

---

### Op√ß√£o 2: Com Docker Compose (Recomendado)

O Docker automatiza todo o ambiente, subindo SQL Server + ETL em containers.

#### 1. Construir e executar:

```bash
docker compose up --build
```

Durante a execu√ß√£o:
1. ‚úÖ SQL Server √© inicializado
2. ‚úÖ Tabelas s√£o criadas automaticamente
3. ‚úÖ Dados fict√≠cios s√£o gerados
4. ‚úÖ Pipeline Bronze √© produzida
5. ‚úÖ Pipeline Silver √© produzida
6. ‚úÖ Logs s√£o exibidos no console

#### 2. Executar componentes isolados:

Subir apenas o SQL Server (manter em background):
```bash
docker compose up -d sqlserver
```

Depois executar a ETL:
```bash
docker compose run --rm etl python etl/etl_sicooperative.py
```

#### 3. Ver logs:

```bash
# Logs da ETL
docker compose logs -f etl

# Logs do SQL Server
docker compose logs -f sqlserver
```

#### 4. Parar os containers:

```bash
docker compose down
```

---

### Op√ß√£o 3: PowerShell Automatizado (Windows)

Se desejar automa√ß√£o completa via script PowerShell:

```powershell
# Permitir execu√ß√£o de scripts (uma vez por sess√£o)
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass

# Executar o fluxo completo
.\run_all.ps1
```

**Nota:** O primeiro build pode demorar v√°rios minutos (PySpark √© grande).

---

## üõ† Tecnologias Utilizadas

| Tecnologia | Uso |
|------------|-----|
| SQL Server | Fonte transacional |
| PySpark | Processamento distribu√≠do |
| Python | Orquestra√ß√µes, gera√ß√£o de dados |
| Docker | Automa√ß√£o e provisionamento |
| Faker | Gera√ß√£o de dados simulados |
| Parquet | Armazenamento Bronze |
| CSV | Entrega Silver |

## üîç Troubleshooting

### A ETL falha por conex√£o com SQL Server
- **venv:** Verifique se o SQL Server est√° rodando localmente
- **Docker:** Aguarde alguns segundos para o SQL Server inicializar (verifique com `docker compose logs sqlserver`)
- Confirme credenciais em `configs.py` ou vari√°veis de ambiente

### Erro ODBC / pyodbc
- erro entre conexao do sqlserver e ubunto no docker, erro se refere ao conector e driver que o ubunto nao tem.

## üìå Observa√ß√µes Importantes

- Estamos simulando um sistema real, onde a aplica√ß√£o consome dados armazenados em SQL Server
- O Docker representa um cen√°rio de ambiente separado (como Produ√ß√£o x Desenvolvimento)
- A coluna data de cria√ß√£o do cart√£o n√£o p√¥de ser implementada porque n√£o existe no modelo fornecido

